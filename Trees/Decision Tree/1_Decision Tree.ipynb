{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook will create the Decision Tree algorithm from scratch. The dataset used for this will be the sklearn diabetes which is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00551455, -0.04464164,  0.04229559, ..., -0.03949338,\n",
       "         0.05227699,  0.02791705],\n",
       "       [ 0.06350368, -0.04464164, -0.05039625, ...,  0.02360753,\n",
       "         0.05803805,  0.04034337],\n",
       "       [ 0.0090156 , -0.04464164,  0.05522933, ...,  0.02323852,\n",
       "         0.05568623,  0.10661708],\n",
       "       ...,\n",
       "       [ 0.03081083, -0.04464164, -0.02021751, ..., -0.03949338,\n",
       "        -0.01090325, -0.0010777 ],\n",
       "       [-0.01277963, -0.04464164, -0.02345095, ..., -0.00259226,\n",
       "        -0.03845972, -0.03835666],\n",
       "       [-0.09269548, -0.04464164,  0.02828403, ..., -0.03949338,\n",
       "        -0.00514219, -0.0010777 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(ds.data)\n",
    "y = np.array(ds.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to make the decision tree. The decision tree separates the data into different leafs. It does this by separating the data using different criteria. For example, on the first iteration we might separate the rows that have the value of index[0] less than 0 to the left leaf and the ones that have more or equal than 0 to the right leaf. But how do we decide where to split the data? \n",
    "\n",
    " Since we don't know the best split criteria, we will have to find it. The most basic way is by iterating through each column and for each column we test every possible value to split and see which one is the best. However, if a column A can have all possible positive values, there would be infinite values for which to try to split the data. So, to circunvent this, for each column, we try to separate the data using only the values we have in our training data. But now, how do we determine what is the best split. There are different metrics that we can use. For example, in the sklearn DecisionTreeRegressor the defalut metric is the mean squared error (MSE). The best split is the one that has the lower MSE. \n",
    "\n",
    "So, let's build 5 functions:\n",
    " 1) Calculates the MSE of a list of values.\n",
    " 2) For a given number of lists (in this case it's always 2) it uses the previous function to calculate the MSE in each list and sums the values of the MSEs.\n",
    " 3) For a given list and a value, it splits the list into 2 lists: one where the values are less than the provided value and the other where the values are equal or bigger.\n",
    " 4) For a given list it calculates the split that results in the least MSE\n",
    " 5) For a matrix X and labels y, it loops through all the columns and executes function 4), calculating the best column and value to split on, returning a dictionary with the index of the column, the value and the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(lst:list)->float:\n",
    "    if len(lst) == 0:\n",
    "        MSE = float('inf')\n",
    "    else:\n",
    "        total = 0\n",
    "        mean = np.mean(lst)\n",
    "        total = sum([(value-mean)**2 for value in lst])\n",
    "        MSE = total/len(lst)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lists_MSEs(list_of_lists:list[list[float]])->float:\n",
    "    MSE_sum = 0\n",
    "    len_all = sum([len(ls) for ls in list_of_lists])\n",
    "    MSE_sum = sum([len(number_list)/len_all*calculate_MSE(number_list) for number_list in list_of_lists])\n",
    "    return MSE_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_indexes(split_value:float, list_column_values:list[float])->list[list[int], list[int]]:\n",
    "    indexes_smaller = []\n",
    "    indexes_eq_bigger = []\n",
    "    for index, value in enumerate(list_column_values):\n",
    "        if value<split_value:\n",
    "            indexes_smaller.append(index)\n",
    "        else:\n",
    "            indexes_eq_bigger.append(index)\n",
    "    return [indexes_smaller, indexes_eq_bigger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_column_MSE(float_list:np.ndarray[float], labels:np.ndarray[float], min_MSE:float)->tuple[float, float]:\n",
    "    best_split = None\n",
    "    for value in float_list:\n",
    "        list_of_lists_of_indexes= calculate_split_indexes(value, float_list)\n",
    "        labels_lists = [labels[indexes] for indexes in list_of_lists_of_indexes]\n",
    "        value_MSE = calculate_lists_MSEs(labels_lists)\n",
    "        if value_MSE< min_MSE:\n",
    "            min_MSE = value_MSE\n",
    "            best_split = value\n",
    "    return (best_split, min_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_best_split(matrix: np.ndarray[float], labels: np.ndarray[float])->dict[str, float]:\n",
    "    result_dictionary = {\n",
    "        \"column_index\":-1,\n",
    "        \"split\":-1,\n",
    "        \"MSE\":calculate_MSE(labels)\n",
    "    }\n",
    "    for column_index in range(matrix.shape[1]):\n",
    "        column_values = matrix[:, column_index] #select all values from the column of index column_index\n",
    "        split_value, MSE = calculate_best_column_MSE(column_values, labels, result_dictionary['MSE'])\n",
    "        if MSE<result_dictionary['MSE']:\n",
    "            result_dictionary['column_index'] = column_index\n",
    "            result_dictionary['split'] = split_value\n",
    "            result_dictionary['MSE'] = MSE\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try to determine the best split for our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data MSE: 6044.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splited dadta MSE: 4288.93\n",
      "Column used to split the data: 2.\n",
      "Value to split the data on: 0.005649978676881689.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data MSE: {round(np.var(y_train), 2)}.\")\n",
    "split_dct = calculate_matrix_best_split(X_train, y_train) \n",
    "print(f\"Splited dadta MSE: {round(split_dct['MSE'], 2)}\")\n",
    "print(f\"Column used to split the data: {split_dct['column_index']}.\")\n",
    "print(f\"Value to split the data on: {split_dct['split']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to keep doing this iteratively. Now the question is: Untill when do we keep doing this? We can do this until we can't do anymore split, (when every leaf node has only one label, multiple labels all with the same value or when the values are different but the feature rows values are all the same), when we reach a maximum number of iterations that we define, or we reach the maximum number of leaves that we define. In this notebook we will build one tree that does the split until we can't do it anymore. In the next ones we will go through different parameters and add them to our Decision tree model. \n",
    "\n",
    "First we have to alter the function calculate_matrix_best_split and calculate_best_column_MSE to also return the splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_column_MSE(float_list:np.ndarray[float], labels:np.ndarray[float], min_MSE:float)->tuple[float, float]:\n",
    "    best_split = None\n",
    "    indexes_1 = np.empty((0))\n",
    "    indexes_2 = np.empty((0))\n",
    "    for value in float_list:\n",
    "        list_of_lists_of_indexes= calculate_split_indexes(value, float_list)\n",
    "        labels_lists = [labels[indexes] for indexes in list_of_lists_of_indexes]\n",
    "        value_MSE = calculate_lists_MSEs(labels_lists)\n",
    "        if value_MSE< min_MSE:\n",
    "            min_MSE = value_MSE\n",
    "            best_split = value\n",
    "            indexes_1 = list_of_lists_of_indexes[0]\n",
    "            indexes_2 = list_of_lists_of_indexes[1]\n",
    "    return (best_split, min_MSE,indexes_1, indexes_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_best_split(matrix: np.ndarray[float], labels: np.ndarray[float])->dict[str, float]:\n",
    "    result_dictionary = {\n",
    "        \"split\":None,\n",
    "        \"MSE\":calculate_MSE(labels),\n",
    "    }\n",
    "    for column_index in range(matrix.shape[1]):\n",
    "        column_values = matrix[:, column_index] #select all values from the column of index column_index\n",
    "        split_value, MSE, indexes_1, indexes_2 = calculate_best_column_MSE(column_values, labels, result_dictionary['MSE'])\n",
    "        if MSE<result_dictionary['MSE']:\n",
    "            result_dictionary['column_index'] = column_index\n",
    "            result_dictionary['split'] = split_value\n",
    "            result_dictionary['MSE'] = MSE\n",
    "            result_dictionary[\"X1\"] = matrix[indexes_1]\n",
    "            result_dictionary[\"X2\"] = matrix[indexes_2]\n",
    "            result_dictionary[\"y1\"] = labels[indexes_1]\n",
    "            result_dictionary[\"y2\"] = labels[indexes_2]\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': 0.005649978676881689,\n",
       " 'MSE': 4288.930696708918,\n",
       " 'column_index': 2,\n",
       " 'X1': array([[ 0.06350368, -0.04464164, -0.05039625, ...,  0.02360753,\n",
       "          0.05803805,  0.04034337],\n",
       "        [-0.08906294, -0.04464164, -0.01159501, ...,  0.03430886,\n",
       "          0.02268774, -0.00936191],\n",
       "        [-0.0382074 , -0.04464164, -0.0105172 , ..., -0.00259226,\n",
       "         -0.01811369, -0.01764613],\n",
       "        ...,\n",
       "        [-0.09632802, -0.04464164, -0.07626374, ..., -0.03949338,\n",
       "         -0.05947118, -0.08391984],\n",
       "        [ 0.03081083, -0.04464164, -0.02021751, ..., -0.03949338,\n",
       "         -0.01090325, -0.0010777 ],\n",
       "        [-0.01277963, -0.04464164, -0.02345095, ..., -0.00259226,\n",
       "         -0.03845972, -0.03835666]]),\n",
       " 'X2': array([[-0.00551455, -0.04464164,  0.04229559, ..., -0.03949338,\n",
       "          0.05227699,  0.02791705],\n",
       "        [ 0.0090156 , -0.04464164,  0.05522933, ...,  0.02323852,\n",
       "          0.05568623,  0.10661708],\n",
       "        [ 0.01628068,  0.05068012,  0.01427248, ...,  0.03430886,\n",
       "          0.07496573,  0.04034337],\n",
       "        ...,\n",
       "        [-0.00188202, -0.04464164,  0.03367309, ..., -0.00259226,\n",
       "          0.02671684,  0.06105391],\n",
       "        [ 0.00538306,  0.05068012,  0.03043966, ..., -0.03949338,\n",
       "          0.0086406 ,  0.01549073],\n",
       "        [-0.09269548, -0.04464164,  0.02828403, ..., -0.03949338,\n",
       "         -0.00514219, -0.0010777 ]]),\n",
       " 'y1': array([189., 206.,  97.,  60.,  61., 128., 104., 132., 283., 129., 257.,\n",
       "        137.,  63.,  93., 179., 262.,  51.,  71.,  69., 154., 116.,  81.,\n",
       "        292.,  55., 107.,  91., 253.,  85., 252.,  59.,  78., 200.,  78.,\n",
       "        245.,  42., 127.,  53.,  94., 104., 199., 248., 170.,  59., 209.,\n",
       "        138., 198., 124.,  96., 101.,  51.,  64., 103.,  86., 111.,  65.,\n",
       "         51.,  48., 109., 178.,  88., 216.,  96., 190.,  74., 160., 196.,\n",
       "         97.,  69., 182., 161., 214.,  45., 150., 160.,  55., 197., 185.,\n",
       "        123.,  72., 185., 144., 168., 151.,  83., 152.,  66., 214.,  85.,\n",
       "        129.,  89., 259., 229., 200.,  77.,  54.,  31., 109., 206., 118.,\n",
       "         83.,  72., 163., 181.,  71., 179., 102., 131.,  47.,  77.,  93.,\n",
       "        162., 183.,  81.,  55., 146., 230.,  40., 135.,  43.,  77.,  49.,\n",
       "         74.,  92.,  84., 144., 142., 115., 158.,  88.,  39.,  80., 217.,\n",
       "         52., 115., 131.,  71., 118.,  25., 100., 200.,  91.,  73.,  66.,\n",
       "         87.,  39.,  92., 292., 142.,  50.,  53., 104.,  75., 120.,  65.,\n",
       "        116.,  95.,  59., 139., 177., 185.,  97.,  42., 241.,  70.,  49.,\n",
       "         44., 191.,  47.,  58., 155.,  79., 104., 143., 152., 170.,  75.,\n",
       "        200., 124.,  49.,  53., 178., 219., 113.,  63., 114., 126.,  88.,\n",
       "         83.,  71., 134.,  65.,  57.,  68., 141., 134., 148.,  64.]),\n",
       " 'y2': array([166., 173., 220., 242., 121., 265., 174., 232., 208., 261., 258.,\n",
       "        237., 139., 268., 317., 249., 192., 122., 259., 191., 210., 175.,\n",
       "        265., 281., 257., 215., 303., 277., 288., 225., 265.,  55., 198.,\n",
       "        252., 220., 131., 212., 142., 155., 121., 131., 128., 141.,  84.,\n",
       "        150.,  60., 279., 182., 245., 276., 174., 180., 150., 138., 246.,\n",
       "        321., 308., 109., 258., 178., 268., 310.,  68., 147., 178., 246.,\n",
       "        127., 332., 109.,  90., 144., 242., 259., 141., 137., 195., 235.,\n",
       "        198., 225., 275., 306., 196., 310., 346., 128., 235., 263., 341.,\n",
       "        273.,  85., 220., 172., 336., 272., 110., 275., 281., 221., 248.,\n",
       "        132.,  67., 202.,  85., 275., 243., 293., 236., 243., 217., 296.,\n",
       "        142., 143.,  99., 233., 164., 145., 201.,  78., 103., 111., 182.,\n",
       "        151.,  91., 163., 283., 200., 113., 274., 311., 244., 173., 270.,\n",
       "        202., 302.])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_matrix_best_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to finish this, we will recursively calculate the best split of the matrix. We stop this until the branch where we are in only has one value in the leaf or when there is no split that will improve the MSE of the values inside the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_best_split(X: np.ndarray, y: np.ndarray)->dict:\n",
    "    tree = {}\n",
    "    best_split_dct = calculate_matrix_best_split(X, y)\n",
    "    if best_split_dct[\"split\"] is None:\n",
    "        tree[\"value\"] = y\n",
    "    else:\n",
    "        branch_1 = (best_split_dct[\"X1\"], best_split_dct[\"y1\"])\n",
    "        branch_2 = (best_split_dct[\"X2\"], best_split_dct[\"y2\"])\n",
    "\n",
    "        tree['column_index'] = best_split_dct['column_index']\n",
    "        tree['split'] = best_split_dct['split']\n",
    "        if len(branch_1[0]) > 1:\n",
    "            tree['left'] = split_data_best_split(branch_1[0], branch_1[1])\n",
    "        else:\n",
    "            tree['left'] = {'value': branch_1[1]}  # Assuming leaf node value\n",
    "        if len(branch_2[0]) > 1:\n",
    "            tree['right'] = split_data_best_split(branch_2[0], branch_2[1])\n",
    "        else:\n",
    "            tree['right'] = {'value': branch_2[1]}  # Assuming leaf node value\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = split_data_best_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we were now able to separate the different samples and reduced the MSE. If we now want to calculate the label y for an observation we just have to follow the tree and reach the leaf node. When we reach it, we calculate the average of the values of the leaf node and that is the value we want! Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x:np.ndarray, tree:dict)->float:\n",
    "    if \"value\" in tree.keys():\n",
    "        return np.mean(tree['value'])\n",
    "    else:\n",
    "        if x[tree['column_index']]<tree[\"split\"]:\n",
    "            return predict(x, tree['left'])\n",
    "        else:\n",
    "            return predict(x, tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_train[0], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! The problem with this tree right now it's that it has overfitted the data that we gave it. The MSE of the training data is perfect but the MSE of the test data will be awful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data MSE: 0.0\n",
      "Test data MSE: 6103.76\n"
     ]
    }
   ],
   "source": [
    "y_pred = [predict(x, res) for x in X_train]\n",
    "error_train = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(y_pred, y_train)) / len(X_train)\n",
    "y_pred = [predict(x, res) for x in X_test]\n",
    "error_test = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(y_pred, y_test)) / len(X_test)\n",
    "error_train\n",
    "\n",
    "print(f\"Train data MSE: {error_train}\")\n",
    "print(f\"Test data MSE: {round(error_test, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But hey, we successfully buit a Decision tree from scratch. Let's now compare with sklearn's decision tree with the same parameters as ours tree and see if the results are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(random_state=42)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tr = DecisionTreeRegressor(random_state=42)\n",
    "tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data MSE: 0.0\n",
      "Test data MSE: 5941.7\n"
     ]
    }
   ],
   "source": [
    "preds = tr.predict(X_train)\n",
    "error_train = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(preds, y_train)) / len(X_train)\n",
    "error_train\n",
    "preds = tr.predict(X_test)\n",
    "error_test = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(preds, y_test)) / len(X_test)\n",
    "error_test\n",
    "print(f\"Train data MSE: {error_train}\")\n",
    "print(f\"Test data MSE: {round(error_test, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much the same result (Although it's much faster)! In the next notebook we change our functions and implement some different parameters to decrease ou overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
