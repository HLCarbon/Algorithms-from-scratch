{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook will create the Decision Tree algorithm from scratch. The dataset used for this will be the sklearn diabetes which is a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ds.data)\n",
    "y = np.array(ds.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to make the decision tree. So let's look at the first step. \n",
    "\n",
    "The first step is to decide how we are going to separate the data. How do we find this? Since we don't know the best spli criteria, we will have to find it. We do this by iterating through each column and for each column we test every possible value to split and see which one is the best. However, if a column A can have all possible positive values, there would be infinite values for which to try to split the data. So, to circunvent this, for each column, we try to separate the labels  using only the values of that column that we found in our training data. But now, how do we determine what is the best split. There are different metrics that we can use, however, in this case, we will calculate the variance of the labels on each child node and sum them. The best split is the one that have the lower variance. \n",
    "\n",
    "So, let's build 5 functions:\n",
    " 1) Calculates the variance of a list of values, returning a float \n",
    " 2) For a given n of labels y it sums the values of the variances, returning a float\n",
    " 3) For a given column c and a list it loops through all its values and calculates the variance of the split caused by the value, returning a tuple with (value, sum of variance)\n",
    " 4) For a given value v, return the indexes that are less than that value and the indexes that are equal or greater than that value, as a list of lists.\n",
    " 5) For a matrix X and labels y, it loops through all the columns and executes function 3), calculating the best column and value to split on, returning a dictionary with the index of the column, the value and the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(lst):\n",
    "    if len(lst) == 0:\n",
    "        variance = float('inf')\n",
    "    else:\n",
    "        total = 0\n",
    "        mean = np.mean(lst)\n",
    "        total = sum([(value-mean)**2 for value in lst])\n",
    "        variance = total/len(lst)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lists_variances(list_of_lists:list[list[float]])->float:\n",
    "    variance_sum = 0\n",
    "    len_all = sum([len(ls) for ls in list_of_lists])\n",
    "    variance_sum = sum([len(number_list)/len_all*calculate_variance(number_list) for number_list in list_of_lists])\n",
    "    return variance_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_indexes(split_value:float, list_column_values:list[float])->list[list[int], list[int]]:\n",
    "    indexes_smaller = []\n",
    "    indexes_eq_bigger = []\n",
    "    for index, value in enumerate(list_column_values):\n",
    "        if value<split_value:\n",
    "            indexes_smaller.append(index)\n",
    "        else:\n",
    "            indexes_eq_bigger.append(index)\n",
    "    return [indexes_smaller, indexes_eq_bigger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_column_variance(float_list:np.ndarray[float], labels:np.ndarray[float], min_variance:float)->tuple[float, float]:\n",
    "    best_split = None\n",
    "    for value in float_list:\n",
    "        list_of_lists_of_indexes= calculate_split_indexes(value, float_list)\n",
    "        labels_lists = [labels[indexes] for indexes in list_of_lists_of_indexes]\n",
    "        value_variance = calculate_lists_variances(labels_lists)\n",
    "        if value_variance< min_variance:\n",
    "            min_variance = value_variance\n",
    "            best_split = value\n",
    "    return (best_split, min_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_best_split(matrix: np.ndarray[float], labels: np.ndarray[float])->dict[str, float]:\n",
    "    result_dictionary = {\n",
    "        \"column_index\":-1,\n",
    "        \"split\":-1,\n",
    "        \"variance\":calculate_variance(labels)\n",
    "    }\n",
    "    for column_index in range(matrix.shape[1]):\n",
    "        column_values = matrix[:, column_index] #select all values from the column of index column_index\n",
    "        split_value, variance = calculate_best_column_variance(column_values, labels, result_dictionary['variance'])\n",
    "        if variance<result_dictionary['variance']:\n",
    "            result_dictionary['column_index'] = column_index\n",
    "            result_dictionary['split'] = split_value\n",
    "            result_dictionary['variance'] = variance\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try to determine the best split for our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column_index': 2,\n",
       " 'split': 0.005649978676881689,\n",
       " 'variance': 4288.930696708918}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_matrix_best_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to keep doing this. Now the question is: Untill when do e keep doing this? We can do this until we can't do anymore split,(when every leaf node has only one label, multiple labels all with the same value or when the values are different but the feature rows values are all the same), when we reach a maximum number of iterations that we define, or we reach the maximum number of leaves that we define."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first build one tree that does the split untill we can't do it anymore and then add the constraints. First we have to alter the function calculate_matrix_best_split and calculate_best_column_variance to also return the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_column_variance(float_list:np.ndarray[float], labels:np.ndarray[float], min_variance:float)->tuple[float, float]:\n",
    "    best_split = None\n",
    "    indexes_1 = np.empty((0))\n",
    "    indexes_2 = np.empty((0))\n",
    "    for value in float_list:\n",
    "        list_of_lists_of_indexes= calculate_split_indexes(value, float_list)\n",
    "        labels_lists = [labels[indexes] for indexes in list_of_lists_of_indexes]\n",
    "        value_variance = calculate_lists_variances(labels_lists)\n",
    "        if value_variance< min_variance:\n",
    "            min_variance = value_variance\n",
    "            best_split = value\n",
    "            indexes_1 = list_of_lists_of_indexes[0]\n",
    "            indexes_2 = list_of_lists_of_indexes[1]\n",
    "    return (best_split, min_variance,indexes_1, indexes_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_best_split(matrix: np.ndarray[float], labels: np.ndarray[float])->dict[str, float]:\n",
    "    result_dictionary = {\n",
    "        \"split\":None,\n",
    "        \"variance\":calculate_variance(labels),\n",
    "    }\n",
    "    for column_index in range(matrix.shape[1]):\n",
    "        column_values = matrix[:, column_index] #select all values from the column of index column_index\n",
    "        split_value, variance, indexes_1, indexes_2 = calculate_best_column_variance(column_values, labels, result_dictionary['variance'])\n",
    "        if variance<result_dictionary['variance']:\n",
    "            result_dictionary['column_index'] = column_index\n",
    "            result_dictionary['split'] = split_value\n",
    "            result_dictionary['variance'] = variance\n",
    "            result_dictionary[\"X1\"] = matrix[indexes_1]\n",
    "            result_dictionary[\"X2\"] = matrix[indexes_2]\n",
    "            result_dictionary[\"y1\"] = labels[indexes_1]\n",
    "            result_dictionary[\"y2\"] = labels[indexes_2]\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': 0.005649978676881689,\n",
       " 'variance': 4288.930696708918,\n",
       " 'column_index': 2,\n",
       " 'X1': array([[ 0.06350368, -0.04464164, -0.05039625, ...,  0.02360753,\n",
       "          0.05803805,  0.04034337],\n",
       "        [-0.08906294, -0.04464164, -0.01159501, ...,  0.03430886,\n",
       "          0.02268774, -0.00936191],\n",
       "        [-0.0382074 , -0.04464164, -0.0105172 , ..., -0.00259226,\n",
       "         -0.01811369, -0.01764613],\n",
       "        ...,\n",
       "        [-0.09632802, -0.04464164, -0.07626374, ..., -0.03949338,\n",
       "         -0.05947118, -0.08391984],\n",
       "        [ 0.03081083, -0.04464164, -0.02021751, ..., -0.03949338,\n",
       "         -0.01090325, -0.0010777 ],\n",
       "        [-0.01277963, -0.04464164, -0.02345095, ..., -0.00259226,\n",
       "         -0.03845972, -0.03835666]]),\n",
       " 'X2': array([[-0.00551455, -0.04464164,  0.04229559, ..., -0.03949338,\n",
       "          0.05227699,  0.02791705],\n",
       "        [ 0.0090156 , -0.04464164,  0.05522933, ...,  0.02323852,\n",
       "          0.05568623,  0.10661708],\n",
       "        [ 0.01628068,  0.05068012,  0.01427248, ...,  0.03430886,\n",
       "          0.07496573,  0.04034337],\n",
       "        ...,\n",
       "        [-0.00188202, -0.04464164,  0.03367309, ..., -0.00259226,\n",
       "          0.02671684,  0.06105391],\n",
       "        [ 0.00538306,  0.05068012,  0.03043966, ..., -0.03949338,\n",
       "          0.0086406 ,  0.01549073],\n",
       "        [-0.09269548, -0.04464164,  0.02828403, ..., -0.03949338,\n",
       "         -0.00514219, -0.0010777 ]]),\n",
       " 'y1': array([189., 206.,  97.,  60.,  61., 128., 104., 132., 283., 129., 257.,\n",
       "        137.,  63.,  93., 179., 262.,  51.,  71.,  69., 154., 116.,  81.,\n",
       "        292.,  55., 107.,  91., 253.,  85., 252.,  59.,  78., 200.,  78.,\n",
       "        245.,  42., 127.,  53.,  94., 104., 199., 248., 170.,  59., 209.,\n",
       "        138., 198., 124.,  96., 101.,  51.,  64., 103.,  86., 111.,  65.,\n",
       "         51.,  48., 109., 178.,  88., 216.,  96., 190.,  74., 160., 196.,\n",
       "         97.,  69., 182., 161., 214.,  45., 150., 160.,  55., 197., 185.,\n",
       "        123.,  72., 185., 144., 168., 151.,  83., 152.,  66., 214.,  85.,\n",
       "        129.,  89., 259., 229., 200.,  77.,  54.,  31., 109., 206., 118.,\n",
       "         83.,  72., 163., 181.,  71., 179., 102., 131.,  47.,  77.,  93.,\n",
       "        162., 183.,  81.,  55., 146., 230.,  40., 135.,  43.,  77.,  49.,\n",
       "         74.,  92.,  84., 144., 142., 115., 158.,  88.,  39.,  80., 217.,\n",
       "         52., 115., 131.,  71., 118.,  25., 100., 200.,  91.,  73.,  66.,\n",
       "         87.,  39.,  92., 292., 142.,  50.,  53., 104.,  75., 120.,  65.,\n",
       "        116.,  95.,  59., 139., 177., 185.,  97.,  42., 241.,  70.,  49.,\n",
       "         44., 191.,  47.,  58., 155.,  79., 104., 143., 152., 170.,  75.,\n",
       "        200., 124.,  49.,  53., 178., 219., 113.,  63., 114., 126.,  88.,\n",
       "         83.,  71., 134.,  65.,  57.,  68., 141., 134., 148.,  64.]),\n",
       " 'y2': array([166., 173., 220., 242., 121., 265., 174., 232., 208., 261., 258.,\n",
       "        237., 139., 268., 317., 249., 192., 122., 259., 191., 210., 175.,\n",
       "        265., 281., 257., 215., 303., 277., 288., 225., 265.,  55., 198.,\n",
       "        252., 220., 131., 212., 142., 155., 121., 131., 128., 141.,  84.,\n",
       "        150.,  60., 279., 182., 245., 276., 174., 180., 150., 138., 246.,\n",
       "        321., 308., 109., 258., 178., 268., 310.,  68., 147., 178., 246.,\n",
       "        127., 332., 109.,  90., 144., 242., 259., 141., 137., 195., 235.,\n",
       "        198., 225., 275., 306., 196., 310., 346., 128., 235., 263., 341.,\n",
       "        273.,  85., 220., 172., 336., 272., 110., 275., 281., 221., 248.,\n",
       "        132.,  67., 202.,  85., 275., 243., 293., 236., 243., 217., 296.,\n",
       "        142., 143.,  99., 233., 164., 145., 201.,  78., 103., 111., 182.,\n",
       "        151.,  91., 163., 283., 200., 113., 274., 311., 244., 173., 270.,\n",
       "        202., 302.])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_matrix_best_split(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_best_split(X: np.ndarray, y: np.ndarray):\n",
    "    tree = {}\n",
    "    best_split_dct = calculate_matrix_best_split(X, y)\n",
    "    if best_split_dct[\"split\"] is None:\n",
    "        tree[\"value\"] = y\n",
    "    else:\n",
    "        branch_1 = (best_split_dct[\"X1\"], best_split_dct[\"y1\"])\n",
    "        branch_2 = (best_split_dct[\"X2\"], best_split_dct[\"y2\"])\n",
    "\n",
    "        tree['column_index'] = best_split_dct['column_index']\n",
    "        tree['split'] = best_split_dct['split']\n",
    "        if len(branch_1[0]) > 1:\n",
    "            tree['left'] = split_data_best_split(branch_1[0], branch_1[1])\n",
    "        else:\n",
    "            tree['left'] = {'value': branch_1[1]}  # Assuming leaf node value\n",
    "        if len(branch_2[0]) > 1:\n",
    "            tree['right'] = split_data_best_split(branch_2[0], branch_2[1])\n",
    "        else:\n",
    "            tree['right'] = {'value': branch_2[1]}  # Assuming leaf node value\n",
    "\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = split_data_best_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we were now able to separate the different samples and reduced the variance. If we now want to calculate the label y for an observation we just have to follow the tree and reach the leaf node. When we reach it, we calculate the average of the values of the leaf node and that is the value we want! Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x:np.ndarray, tree:dict)->float:\n",
    "    if \"value\" in tree.keys():\n",
    "        return np.mean(tree['value'])\n",
    "    else:\n",
    "        if x[tree['column_index']]<tree[\"split\"]:\n",
    "            return predict(x, tree['left'])\n",
    "        else:\n",
    "            return predict(x, tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_train[0], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's pretty bad, but it works! Why is it so bad? Because it is overfitted with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [predict(x, res) for x in X_train]\n",
    "error_train = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(y_pred, y_train)) / len(X_train)\n",
    "error_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6103.756756756757"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [predict(x, res) for x in X_test]\n",
    "error_test = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(y_pred, y_test)) / len(X_test)\n",
    "error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tr = DecisionTreeRegressor(random_state=42)\n",
    "tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tr.predict(X_train)\n",
    "error_train = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(preds, y_train)) / len(X_train)\n",
    "error_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5941.7027027027025"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tr.predict(X_test)\n",
    "error_test = sum((y_train_pred -y_true)**2 for y_train_pred, y_true in zip(preds, y_test)) / len(X_test)\n",
    "error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
